{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "monty.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "TPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "KQ5pCgzENGno",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import os, pkgutil, numpy as np, re, shutil\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from collections import OrderedDict\n",
    "from io import BytesIO\n",
    "from tokenize import tokenize, STRING, INDENT, DEDENT\n",
    "\n",
    "DEBUG = False"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_file(path, verbose=False):\n",
    "    \"\"\"\n",
    "    Read data from a file and return its contents in a string.\n",
    "    :param path: str, path to file's location\n",
    "    :param verbose: bool, whether to print error message\n",
    "    :return: str, file's content or empty string if file not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(path, 'r') as file:\n",
    "            return file.read()\n",
    "    except FileNotFoundError:\n",
    "        if verbose:\n",
    "            print('INCORRECT FILE PATH:', path)\n",
    "        return ''\n",
    "\n",
    "\n",
    "def write_file(data, path, append=False):\n",
    "    \"\"\"\n",
    "    Write information provided into file. overwrites all existing data and creates new file if necessary.\n",
    "    :param data: str, information to write to file\n",
    "    :param path: path to data's destination\n",
    "    :param append: bool, whether to append or overwrite file\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    mode = 'w'\n",
    "    if append:\n",
    "        mode = 'a'\n",
    "\n",
    "    with open(path, mode) as file:\n",
    "        file.write(data)\n",
    "\n",
    "\n",
    "def copy_file(source_path, dest_path):\n",
    "    \"\"\"\n",
    "    Copies the content of a source file to either another arbitrary file path or to an index in the buffer.\n",
    "    :param source_path: str, path to the source file\n",
    "    :param dest_path: str, path to files destination\n",
    "    :return: bool, success or failure\n",
    "    \"\"\"\n",
    "    data = read_file(source_path)\n",
    "    if data:\n",
    "        write_file(data, dest_path)\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "\n",
    "def get_dir_length(path):\n",
    "    \"\"\"\n",
    "    Gets number of files in buffer.\n",
    "    :return: int, number of files in buffer directory\n",
    "    \"\"\"\n",
    "    return len([0 for name in os.listdir(path) if os.path.isfile(name)])\n",
    "\n",
    "\n",
    "def get_importable_modules():\n",
    "    \"\"\"\n",
    "    get a list of all importable modules in current venv.\n",
    "    :return: list, list of strs, each of which is the name of an importable module\n",
    "    \"\"\"\n",
    "    modules = []\n",
    "    for pkg in pkgutil.iter_modules():\n",
    "        modules.append(pkg.name)\n",
    "\n",
    "    return modules\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PyTokenizer:\n",
    "    def __init__(self, max_vocab_len):\n",
    "        \"\"\"\n",
    "        Create a tokenizer for python scripts.\n",
    "        :param max_vocab_len: int, maximum size of vocabulary length. Actual length may be less\n",
    "        \"\"\"\n",
    "        self.max_vocab_len = max_vocab_len\n",
    "        self.word_idx = {}\n",
    "        self.idx_word = {}\n",
    "\n",
    "    def fit_on_data(self, data):\n",
    "        \"\"\"\n",
    "        Create token index from data.\n",
    "        :param data: str, corpus to create tokenizer on\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        tokens = PyTokenizer.py_tokenize(data)\n",
    "        word_counts = OrderedDict()\n",
    "        for t in tokens:\n",
    "            if t in word_counts:\n",
    "                word_counts[t] += 1\n",
    "            else:\n",
    "                word_counts[t] = 1\n",
    "\n",
    "        wcounts = list(word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        wcounts.insert(0, ['OOV', None])\n",
    "\n",
    "        if len(wcounts) > self.max_vocab_len:\n",
    "            wcounts = wcounts[:self.max_vocab_len]\n",
    "\n",
    "        self.word_idx = dict(zip([wc[0] for wc in wcounts], list(range(len(wcounts)))))\n",
    "        self.idx_word = dict(zip(list(range(1, len(wcounts) + 1)), [wc[0] for wc in wcounts]))\n",
    "\n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"\n",
    "        Convert string to sequence of token indices.\n",
    "        :param text: str, text to tokenize\n",
    "        :return: list, list of token indices\n",
    "        \"\"\"\n",
    "        tokens = PyTokenizer.py_tokenize(text)\n",
    "        return [self.word_idx.get(t, 1) for t in tokens]\n",
    "\n",
    "    def sequence_to_text(self, seq):\n",
    "        \"\"\"\n",
    "        Convert list of token indices to python string.\n",
    "        :param seq: list, list of integer indices\n",
    "        :return: str, joined token list\n",
    "        \"\"\"\n",
    "        string_tokens = [self.idx_word.get(i, 'OOV') for i in seq]\n",
    "        print(string_tokens, '\\n\\n')\n",
    "        return PyTokenizer.py_untokenize(string_tokens)\n",
    "\n",
    "    @property\n",
    "    def real_vocab_len(self):\n",
    "        \"\"\"\n",
    "        Get actual length of vocabulary\n",
    "        :return: int, actual vocab length\n",
    "        \"\"\"\n",
    "        return len(self.word_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def py_tokenize(data):\n",
    "        \"\"\"\n",
    "        Convert py string into tokens.\n",
    "        :param data: str, python script\n",
    "        :return: list, list of string tokens.\n",
    "        \"\"\"\n",
    "        token_generator = tokenize(BytesIO(data.encode('utf-8')).readline)\n",
    "        tokens = []\n",
    "        print_next = False\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                token_type, val, start, end, line = next(token_generator)\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "            if DEBUG and (\"No data provided for\" in val or (print_next and i < 40)):\n",
    "                print('In function py_tokenize. TOKEN_TYPE:', token_type, 'VALUE:', val, 'START_POS:', start,\n",
    "                      'END_POS:', end, 'FULL_LINE:', line[:-1])\n",
    "                print_next = True\n",
    "                i += 1\n",
    "\n",
    "            if token_type == STRING:\n",
    "                if val[0] != '\"' and val[0] != \"'\":\n",
    "                    str_contents = val[2:-1].split(' ')\n",
    "                else:\n",
    "                    str_contents = val[1:-1].split(' ')\n",
    "                str_contents = [t for t in str_contents if t]\n",
    "                tokens.extend([\"'\", *str_contents, \"'\"])\n",
    "            elif token_type == INDENT:\n",
    "                tokens.append('INDENT')\n",
    "            elif token_type == DEDENT:\n",
    "                tokens.append('DEDENT')\n",
    "            elif val == 'utf-8':\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(val)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def py_untokenize(tokens):\n",
    "        \"\"\"\n",
    "        Convert list of string tokens to single python script string\n",
    "        :param tokens: list, list of strings\n",
    "        :return: str, joined tokens\n",
    "        \"\"\"\n",
    "        joined_tokens = ''\n",
    "        indent = 0\n",
    "        cont_str = False\n",
    "        str_buffer = ''\n",
    "        start_line = False\n",
    "        num_lines = 1\n",
    "        for i, t in enumerate(tokens):\n",
    "            if start_line and t != 'INDENT' and t != 'DEDENT':\n",
    "                joined_tokens += ' ' * indent\n",
    "                start_line = False\n",
    "            elif t == 'INDENT':\n",
    "                indent += 4\n",
    "                continue\n",
    "            elif t == 'DEDENT':\n",
    "                indent = max(0, indent - 4)\n",
    "                continue\n",
    "\n",
    "            if cont_str:\n",
    "                if t == '\"' or t == \"'\":\n",
    "                    joined_tokens += str_buffer + t + ' '\n",
    "                    str_buffer = ''\n",
    "                    cont_str = False\n",
    "                elif t == 'EOF':\n",
    "                    if DEBUG:\n",
    "                        print('ERROR: OPEN STRING WHEN EOF REACHED @ token', i, '@ line', num_lines,\n",
    "                              'in Function: py_untokenize')\n",
    "                    joined_tokens += str_buffer[0] * 2 + '\\nEOF\\n'\n",
    "                    last_10 = joined_tokens[-10:]\n",
    "                    str_buffer = ''\n",
    "                    cont_str = False\n",
    "                else:\n",
    "                    str_buffer += t + ' '\n",
    "            elif t == \"'\" or t == '\"':\n",
    "                str_buffer += \"'\"\n",
    "                cont_str = True\n",
    "            elif t == '\\n':\n",
    "                start_line = True\n",
    "                joined_tokens += '\\n'\n",
    "            elif t == 'EOF':\n",
    "                num_lines += 1\n",
    "                continue\n",
    "            else:\n",
    "                joined_tokens += t + ' '\n",
    "\n",
    "        return joined_tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, buffer_dir, clean_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Create processor.\n",
    "        :param buffer_dir: str, path to buffer directory\n",
    "        :param clean_file: str, path to cleaned data file\n",
    "        :param tokenizer: Tokenizer, tokenizer object\n",
    "        \"\"\"\n",
    "        self.buffer_dir = buffer_dir\n",
    "        self.clean_file = clean_file\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_training_data(self, sample_len=50, step=1, one_hot_input=False):\n",
    "        \"\"\"\n",
    "        get training data in form necessary for model training.\n",
    "        :param sample_len: int, length of the samples to generate\n",
    "        :param step: step to travel training sequence with\n",
    "        :param one_hot_input: bool, whether to convert input to one hot vectors or not\n",
    "        :return x: np.array, training inputs w/ dim (#samples, sample_len)\n",
    "        :return y: np.array, training labels w/ dim (#samples, vocab_len)\n",
    "        \"\"\"\n",
    "        if self.tokenizer.real_vocab_len == 0:\n",
    "            data = read_file(self.clean_file)\n",
    "            self.tokenizer.fit_on_data(data)\n",
    "\n",
    "        data = read_file(self.clean_file, True)\n",
    "        tokens = self.tokenizer.text_to_sequence(data)\n",
    "\n",
    "        statements = []\n",
    "        next_statements = []\n",
    "        for i in range(0, len(tokens) - sample_len, step):\n",
    "            statements.append(tokens[i:i + sample_len])\n",
    "            next_statements.append(tokens[i + sample_len])\n",
    "\n",
    "        if one_hot_input:\n",
    "            raise NotImplementedError(\"Need to implement this\")\n",
    "        else:\n",
    "            x = np.array(statements, dtype=np.int)\n",
    "\n",
    "        y = np.zeros((len(statements), self.tokenizer.real_vocab_len), dtype=np.int)\n",
    "        for i, next_statement in enumerate(next_statements):  # one hots y matrix\n",
    "            y[i, next_statement] = 1\n",
    "\n",
    "        if DEBUG:\n",
    "            print('x shape:', x.shape, 'y shape:', y.shape)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    # DATA CLEANSING\n",
    "\n",
    "    def clean_buffer(self, empty=True, append=True):\n",
    "        \"\"\"\n",
    "        clean all files in buffer and add to cleaned data file and empty buffer if necessary.\n",
    "        :param empty: bool, whether to empty buffer or not.\n",
    "        :param append: bool, whether to append cleaned data into clean file or overwrite it.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        write_file('', self.clean_file, append)  # clear old file if necessary\n",
    "        for i, file in enumerate(os.listdir(self.buffer_dir)):\n",
    "            path = os.path.join(self.buffer_dir, file)\n",
    "            data = read_file(path)\n",
    "            if not data:\n",
    "                os.remove(path)\n",
    "                continue\n",
    "            try:\n",
    "                clean_data = self.process_text(data)\n",
    "                write_file(clean_data, self.clean_file, True)\n",
    "            except Exception:\n",
    "                if DEBUG:\n",
    "                    print('Error found tokenizing', path)\n",
    "\n",
    "        if empty:\n",
    "            shutil.rmtree(self.buffer_dir)\n",
    "            os.mkdir(self.buffer_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def process_text(data):\n",
    "        \"\"\"\n",
    "        Clean a text string of python code.\n",
    "        :param data: str, python code\n",
    "        :return: str, cleaned python code\n",
    "        \"\"\"\n",
    "        # delete comments\n",
    "        def comment_subber(match_obj):\n",
    "            string = match_obj.group(0)\n",
    "            if string.startswith(\"'''\") or string.startswith('\"\"\"') or string.startswith('#'):\n",
    "                return ''\n",
    "            return string\n",
    "\n",
    "        comment_pattern = '\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\'|\"(\\\\[\\s\\S]|[^\"])*\"|\\'(\\\\[\\s\\S]|[^\\'])*\\'|#[\\s\\S]*'\n",
    "        data = re.compile(comment_pattern, re.DOTALL).sub(comment_subber, data)\n",
    "\n",
    "        # remove imports\n",
    "        data = re.sub('(\\n|^)(import|from).*', '', data)\n",
    "\n",
    "        # add special EOF token\n",
    "        data = re.sub('[\\n\\s]*EOF[\\n\\s]*', '', data)  # deletes any old EOF tokens\n",
    "        data += '\\nEOF\\n'\n",
    "\n",
    "        # remove unnecessary newlines\n",
    "        data = PreProcessor.remove_newlines(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_newlines(data):\n",
    "        \"\"\"\n",
    "        Remove unnecessary newlines from python string\n",
    "        :param data: str, python code\n",
    "        :return: str, cleaned python code\n",
    "        \"\"\"\n",
    "        data = re.sub(r'\\n[\\n\\s]*\\n', '\\n', data)\n",
    "        while data[0] == '\\n':  # check for newline @ file start\n",
    "            data = data[1:]\n",
    "\n",
    "        return data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, vocab_len, sample_len, **hyper_params):\n",
    "        self.vocab_len = vocab_len\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(vocab_len, 512))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(128))\n",
    "        self.model.add(Dense(vocab_len, activation='softmax'))\n",
    "\n",
    "        optimizer = RMSprop(lr=1e-5)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, x, y, epochs=1, mini_batch_size=128):\n",
    "        \"\"\"\n",
    "        train model on given data\n",
    "        :param x: np.array, training inputs, dims (#samples, sample_len)\n",
    "        :param y: np.array, training labels, dims (#samples, vocab_len)\n",
    "        :param epochs: int, number of iterations to train on data\n",
    "        :param mini_batch_size: int, size of mini batches\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        assert x.shape[1] == self.sample_len, 'Incorrect sample length. Given: {}, Expecting: {}'.format(\n",
    "            x.shape[1], self.sample_len)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\"best_model\",\n",
    "                                     monitor='loss',\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True,\n",
    "                                     mode='auto',\n",
    "                                     period=1)\n",
    "\n",
    "        self.model.fit(x, y, mini_batch_size, epochs, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "    def generate_script(self, seed, temp=0.5, **stop):\n",
    "        \"\"\"\n",
    "        generate a script of certain length or until a token idx is reached.\n",
    "        :param seed: np.array, input into model to generate sample from, dims (1, sample_len)\n",
    "        :param temp: float, softmax temperature, amount of entropy to include in sample\n",
    "        :param stop: kwargs, either len (int), number of new tokens to generate; or token (int) idx of token to stop at\n",
    "        :return: list, full sequence generated as list of token indices (includes seed)\n",
    "        \"\"\"\n",
    "        assert seed.shape[1] == self.sample_len, 'Incorrect sample length. Given: {}, Expecting: {}'.format(\n",
    "            seed.shape[1], self.sample_len)\n",
    "        generated_sequence = list(seed[0])\n",
    "\n",
    "        while True:\n",
    "            pred = self.model.predict(seed, verbose=0)[0]\n",
    "            next_idx = self.sample_next_token(pred, temp)\n",
    "            generated_sequence.append(next_idx)\n",
    "            if stop.get('len', -1) == len(generated_sequence) - self.sample_len or stop.get('token', -1) == next_idx:\n",
    "                break\n",
    "            seed[0] = Model.np_shift(seed[0], -1)\n",
    "            seed[0, -1] = next_idx\n",
    "\n",
    "        return generated_sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(seed, temp):\n",
    "        \"\"\"\n",
    "        sample next token given model's output sequence.\n",
    "        :param seed: np.array, output from model used to pick next token\n",
    "        :param temp: float, amount of randomness to use when sampling next token\n",
    "        :return: int, index of sampled token\n",
    "        \"\"\"\n",
    "        preds = np.asarray(seed).astype('float64')\n",
    "        preds = np.log(preds) / temp\n",
    "        exp_preds = np.exp(preds)\n",
    "        probs = np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1)\n",
    "        return np.argmax(probs)\n",
    "\n",
    "    @staticmethod\n",
    "    def np_shift(xs, n):\n",
    "        if n >= 0:\n",
    "            return np.concatenate((np.full(n, np.nan), xs[:-n]))\n",
    "        else:\n",
    "            return np.concatenate((xs[-n:], np.full(-n, np.nan)))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "t = PyTokenizer(5000)\n",
    "p = PreProcessor('', 'clean.py', t)\n",
    "x, y = p.get_training_data(50)\n",
    "\n",
    "m = Model(t.real_vocab_len, 50)\n",
    "m.train(x, y, 20, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!zip -r ./best_model.zip ./best_model/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def generate_seed(tokenizer, sample_len):\n",
    "    \"\"\"\n",
    "    Get seed from cleaned file.\n",
    "    :param tokenizer: PyTokenizer, tokenizer to use.\n",
    "    :param sample_len: int, length of seed to generate\n",
    "    :return: np.array, model seed shape (1, sample_len)\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    with open('clean.py', 'r') as file:\n",
    "        while len(tokens) < sample_len:\n",
    "            tokens.extend(tokenizer.text_to_sequence(file.readline()))\n",
    "            print(len(tokens))\n",
    "\n",
    "    if len(tokens) != sample_len:\n",
    "        tokens = tokens[:sample_len]\n",
    "\n",
    "    return np.asarray(tokens, dtype=np.int).reshape((1, sample_len))\n",
    "\n",
    "m.model = load_model('best_model')\n",
    "print(m.sample_len)\n",
    "script = m.generate_script(generate_seed(t, m.sample_len), token=t.word_idx['EOF'])\n",
    "print(t.sequence_to_text(script))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "m.model = load_model('best_model')\n",
    "m.train(x, y, 20, 64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gc7sftAMOI7T",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "m.model = load_model('best_model')\n",
    "m.train(x, y, 20, 64)"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0baJ-B9_NRQ9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class PyTokenizer:\n",
    "    def __init__(self, max_vocab_len):\n",
    "        \"\"\"\n",
    "        Create a tokenizer for python scripts.\n",
    "        :param max_vocab_len: int, maximum size of vocabulary length. Actual length may be less\n",
    "        \"\"\"\n",
    "        self.max_vocab_len = max_vocab_len\n",
    "        self.word_idx = {}\n",
    "        self.idx_word = {}\n",
    "\n",
    "    def fit_on_data(self, data):\n",
    "        \"\"\"\n",
    "        Create token index from data.\n",
    "        :param data: str, corpus to create tokenizer on\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        tokens = PyTokenizer.py_tokenize(data)\n",
    "        word_counts = OrderedDict()\n",
    "        for t in tokens:\n",
    "            if t in word_counts:\n",
    "                word_counts[t] += 1\n",
    "            else:\n",
    "                word_counts[t] = 1\n",
    "\n",
    "        wcounts = list(word_counts.items())\n",
    "        wcounts.sort(key=lambda x: x[1], reverse=True)\n",
    "        wcounts.insert(0, ['OOV', None])\n",
    "\n",
    "        if len(wcounts) > self.max_vocab_len:\n",
    "            wcounts = wcounts[:self.max_vocab_len]\n",
    "\n",
    "        self.word_idx = dict(zip([wc[0] for wc in wcounts], list(range(len(wcounts)))))\n",
    "        self.idx_word = dict(zip(list(range(1, len(wcounts) + 1)), [wc[0] for wc in wcounts]))\n",
    "\n",
    "    def text_to_sequence(self, text):\n",
    "        \"\"\"\n",
    "        Convert string to sequence of token indices.\n",
    "        :param text: str, text to tokenize\n",
    "        :return: list, list of token indices\n",
    "        \"\"\"\n",
    "        tokens = PyTokenizer.py_tokenize(text)\n",
    "        return [self.word_idx.get(t, 1) for t in tokens]\n",
    "\n",
    "    def sequence_to_text(self, seq):\n",
    "        \"\"\"\n",
    "        Convert list of token indices to python string.\n",
    "        :param seq: list, list of integer indices\n",
    "        :return: str, joined token list\n",
    "        \"\"\"\n",
    "        string_tokens = [self.idx_word.get(i, 'OOV') for i in seq]\n",
    "        print(string_tokens, '\\n\\n')\n",
    "        return PyTokenizer.py_untokenize(string_tokens)\n",
    "\n",
    "    @property\n",
    "    def real_vocab_len(self):\n",
    "        \"\"\"\n",
    "        Get actual length of vocabulary\n",
    "        :return: int, actual vocab length\n",
    "        \"\"\"\n",
    "        return len(self.word_idx)\n",
    "\n",
    "    @staticmethod\n",
    "    def py_tokenize(data):\n",
    "        \"\"\"\n",
    "        Convert py string into tokens.\n",
    "        :param data: str, python script\n",
    "        :return: list, list of string tokens.\n",
    "        \"\"\"\n",
    "        token_generator = tokenize(BytesIO(data.encode('utf-8')).readline)\n",
    "        tokens = []\n",
    "        print_next = False\n",
    "        i = 0\n",
    "        while True:\n",
    "            try:\n",
    "                token_type, val, start, end, line = next(token_generator)\n",
    "            except Exception:\n",
    "                break\n",
    "\n",
    "            if DEBUG and (\"No data provided for\" in val or (print_next and i < 40)):\n",
    "                print('In function py_tokenize. TOKEN_TYPE:', token_type, 'VALUE:', val, 'START_POS:', start,\n",
    "                      'END_POS:', end, 'FULL_LINE:', line[:-1])\n",
    "                print_next = True\n",
    "                i += 1\n",
    "\n",
    "            if token_type == STRING:\n",
    "                if val[0] != '\"' and val[0] != \"'\":\n",
    "                    str_contents = val[2:-1].split(' ')\n",
    "                else:\n",
    "                    str_contents = val[1:-1].split(' ')\n",
    "                str_contents = [t for t in str_contents if t]\n",
    "                tokens.extend([\"'\", *str_contents, \"'\"])\n",
    "            elif token_type == INDENT:\n",
    "                tokens.append('INDENT')\n",
    "            elif token_type == DEDENT:\n",
    "                tokens.append('DEDENT')\n",
    "            elif val == 'utf-8':\n",
    "                continue\n",
    "            else:\n",
    "                tokens.append(val)\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def py_untokenize(tokens):\n",
    "        \"\"\"\n",
    "        Convert list of string tokens to single python script string\n",
    "        :param tokens: list, list of strings\n",
    "        :return: str, joined tokens\n",
    "        \"\"\"\n",
    "        joined_tokens = ''\n",
    "        indent = 0\n",
    "        cont_str = False\n",
    "        str_buffer = ''\n",
    "        start_line = False\n",
    "        num_lines = 1\n",
    "        for i, t in enumerate(tokens):\n",
    "            if start_line and t != 'INDENT' and t != 'DEDENT':\n",
    "                joined_tokens += ' ' * indent\n",
    "                start_line = False\n",
    "            elif t == 'INDENT':\n",
    "                indent += 4\n",
    "                continue\n",
    "            elif t == 'DEDENT':\n",
    "                indent = max(0, indent - 4)\n",
    "                continue\n",
    "\n",
    "            if cont_str:\n",
    "                if t == '\"' or t == \"'\":\n",
    "                    joined_tokens += str_buffer + t + ' '\n",
    "                    str_buffer = ''\n",
    "                    cont_str = False\n",
    "                elif t == 'EOF':\n",
    "                    if DEBUG:\n",
    "                        print('ERROR: OPEN STRING WHEN EOF REACHED @ token', i, '@ line', num_lines,\n",
    "                              'in Function: py_untokenize')\n",
    "                    joined_tokens += str_buffer[0] * 2 + '\\nEOF\\n'\n",
    "                    last_10 = joined_tokens[-10:]\n",
    "                    str_buffer = ''\n",
    "                    cont_str = False\n",
    "                else:\n",
    "                    str_buffer += t + ' '\n",
    "            elif t == \"'\" or t == '\"':\n",
    "                str_buffer += \"'\"\n",
    "                cont_str = True\n",
    "            elif t == '\\n':\n",
    "                start_line = True\n",
    "                joined_tokens += '\\n'\n",
    "            elif t == 'EOF':\n",
    "                num_lines += 1\n",
    "                continue\n",
    "            else:\n",
    "                joined_tokens += t + ' '\n",
    "\n",
    "        return joined_tokens"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "irymVbVUNion",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self, buffer_dir, clean_file, tokenizer):\n",
    "        \"\"\"\n",
    "        Create processor.\n",
    "        :param buffer_dir: str, path to buffer directory\n",
    "        :param clean_file: str, path to cleaned data file\n",
    "        :param tokenizer: Tokenizer, tokenizer object\n",
    "        \"\"\"\n",
    "        self.buffer_dir = buffer_dir\n",
    "        self.clean_file = clean_file\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def get_training_data(self, sample_len=50, step=1, one_hot_input=False):\n",
    "        \"\"\"\n",
    "        get training data in form necessary for model training.\n",
    "        :param sample_len: int, length of the samples to generate\n",
    "        :param step: step to travel training sequence with\n",
    "        :param one_hot_input: bool, whether to convert input to one hot vectors or not\n",
    "        :return x: np.array, training inputs w/ dim (#samples, sample_len)\n",
    "        :return y: np.array, training labels w/ dim (#samples, vocab_len)\n",
    "        \"\"\"\n",
    "        if self.tokenizer.real_vocab_len == 0:\n",
    "            data = read_file(self.clean_file)\n",
    "            self.tokenizer.fit_on_data(data)\n",
    "\n",
    "        data = read_file(self.clean_file, True)\n",
    "        tokens = self.tokenizer.text_to_sequence(data)\n",
    "\n",
    "        statements = []\n",
    "        next_statements = []\n",
    "        for i in range(0, len(tokens) - sample_len, step):\n",
    "            statements.append(tokens[i:i + sample_len])\n",
    "            next_statements.append(tokens[i + sample_len])\n",
    "\n",
    "        if one_hot_input:\n",
    "            raise NotImplementedError(\"Need to implement this\")\n",
    "        else:\n",
    "            x = np.array(statements, dtype=np.int)\n",
    "\n",
    "        y = np.zeros((len(statements), self.tokenizer.real_vocab_len), dtype=np.int)\n",
    "        for i, next_statement in enumerate(next_statements):  # one hots y matrix\n",
    "            y[i, next_statement] = 1\n",
    "\n",
    "        if DEBUG:\n",
    "            print('x shape:', x.shape, 'y shape:', y.shape)\n",
    "\n",
    "        return x, y\n",
    "\n",
    "    # DATA CLEANSING\n",
    "\n",
    "    def clean_buffer(self, empty=True, append=True):\n",
    "        \"\"\"\n",
    "        clean all files in buffer and add to cleaned data file and empty buffer if necessary.\n",
    "        :param empty: bool, whether to empty buffer or not.\n",
    "        :param append: bool, whether to append cleaned data into clean file or overwrite it.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        write_file('', self.clean_file, append)  # clear old file if necessary\n",
    "        for i, file in enumerate(os.listdir(self.buffer_dir)):\n",
    "            path = os.path.join(self.buffer_dir, file)\n",
    "            data = read_file(path)\n",
    "            if not data:\n",
    "                os.remove(path)\n",
    "                continue\n",
    "            try:\n",
    "                clean_data = self.process_text(data)\n",
    "                write_file(clean_data, self.clean_file, True)\n",
    "            except Exception:\n",
    "                if DEBUG:\n",
    "                    print('Error found tokenizing', path)\n",
    "\n",
    "        if empty:\n",
    "            shutil.rmtree(self.buffer_dir)\n",
    "            os.mkdir(self.buffer_dir)\n",
    "\n",
    "    @staticmethod\n",
    "    def process_text(data):\n",
    "        \"\"\"\n",
    "        Clean a text string of python code.\n",
    "        :param data: str, python code\n",
    "        :return: str, cleaned python code\n",
    "        \"\"\"\n",
    "        # delete comments\n",
    "        def comment_subber(match_obj):\n",
    "            string = match_obj.group(0)\n",
    "            if string.startswith(\"'''\") or string.startswith('\"\"\"') or string.startswith('#'):\n",
    "                return ''\n",
    "            return string\n",
    "\n",
    "        comment_pattern = '\"\"\".*?\"\"\"|\\'\\'\\'.*?\\'\\'\\'|\"(\\\\[\\s\\S]|[^\"])*\"|\\'(\\\\[\\s\\S]|[^\\'])*\\'|#[\\s\\S]*'\n",
    "        data = re.compile(comment_pattern, re.DOTALL).sub(comment_subber, data)\n",
    "\n",
    "        # remove imports\n",
    "        data = re.sub('(\\n|^)(import|from).*', '', data)\n",
    "\n",
    "        # add special EOF token\n",
    "        data = re.sub('[\\n\\s]*EOF[\\n\\s]*', '', data)  # deletes any old EOF tokens\n",
    "        data += '\\nEOF\\n'\n",
    "\n",
    "        # remove unnecessary newlines\n",
    "        data = PreProcessor.remove_newlines(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "    @staticmethod\n",
    "    def remove_newlines(data):\n",
    "        \"\"\"\n",
    "        Remove unnecessary newlines from python string\n",
    "        :param data: str, python code\n",
    "        :return: str, cleaned python code\n",
    "        \"\"\"\n",
    "        data = re.sub(r'\\n[\\n\\s]*\\n', '\\n', data)\n",
    "        while data[0] == '\\n':  # check for newline @ file start\n",
    "            data = data[1:]\n",
    "\n",
    "        return data"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mZ83BB8lNqGC",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "class Model:\n",
    "    def __init__(self, vocab_len, sample_len, **hyper_params):\n",
    "        self.vocab_len = vocab_len\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(vocab_len, 512))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(300, return_sequences=True))\n",
    "        self.model.add(LSTM(128))\n",
    "        self.model.add(Dense(vocab_len, activation='softmax'))\n",
    "\n",
    "        optimizer = RMSprop(lr=1e-5)\n",
    "        self.model.compile(loss='categorical_crossentropy',\n",
    "                           optimizer=optimizer,\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def train(self, x, y, epochs=1, mini_batch_size=128):\n",
    "        \"\"\"\n",
    "        train model on given data\n",
    "        :param x: np.array, training inputs, dims (#samples, sample_len)\n",
    "        :param y: np.array, training labels, dims (#samples, vocab_len)\n",
    "        :param epochs: int, number of iterations to train on data\n",
    "        :param mini_batch_size: int, size of mini batches\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        assert x.shape[1] == self.sample_len, 'Incorrect sample length. Given: {}, Expecting: {}'.format(\n",
    "            x.shape[1], self.sample_len)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\"best_model\",\n",
    "                                     monitor='loss',\n",
    "                                     verbose=1,\n",
    "                                     save_best_only=True,\n",
    "                                     mode='auto',\n",
    "                                     period=1)\n",
    "\n",
    "        self.model.fit(x, y, mini_batch_size, epochs, verbose=1, callbacks=[checkpoint])\n",
    "\n",
    "    def generate_script(self, seed, temp=0.5, **stop):\n",
    "        \"\"\"\n",
    "        generate a script of certain length or until a token idx is reached.\n",
    "        :param seed: np.array, input into model to generate sample from, dims (1, sample_len)\n",
    "        :param temp: float, softmax temperature, amount of entropy to include in sample\n",
    "        :param stop: kwargs, either len (int), number of new tokens to generate; or token (int) idx of token to stop at\n",
    "        :return: list, full sequence generated as list of token indices (includes seed)\n",
    "        \"\"\"\n",
    "        assert seed.shape[1] == self.sample_len, 'Incorrect sample length. Given: {}, Expecting: {}'.format(\n",
    "            seed.shape[1], self.sample_len)\n",
    "        generated_sequence = list(seed[0])\n",
    "\n",
    "        while True:\n",
    "            pred = self.model.predict(seed, verbose=0)[0]\n",
    "            next_idx = self.sample_next_token(pred, temp)\n",
    "            generated_sequence.append(next_idx)\n",
    "            if stop.get('len', -1) == len(generated_sequence) - self.sample_len or stop.get('token', -1) == next_idx:\n",
    "                break\n",
    "            seed[0] = Model.np_shift(seed[0], -1)\n",
    "            seed[0, -1] = next_idx\n",
    "\n",
    "        return generated_sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def sample_next_token(seed, temp):\n",
    "        \"\"\"\n",
    "        sample next token given model's output sequence.\n",
    "        :param seed: np.array, output from model used to pick next token\n",
    "        :param temp: float, amount of randomness to use when sampling next token\n",
    "        :return: int, index of sampled token\n",
    "        \"\"\"\n",
    "        preds = np.asarray(seed).astype('float64')\n",
    "        preds = np.log(preds) / temp\n",
    "        exp_preds = np.exp(preds)\n",
    "        probs = np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1)\n",
    "        return np.argmax(probs)\n",
    "\n",
    "    @staticmethod\n",
    "    def np_shift(xs, n):\n",
    "        if n >= 0:\n",
    "            return np.concatenate((np.full(n, np.nan), xs[:-n]))\n",
    "        else:\n",
    "            return np.concatenate((xs[-n:], np.full(-n, np.nan)))\n"
   ],
   "execution_count": 0,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "u8CBevbtNtXV",
    "colab_type": "code",
    "outputId": "6d5bbd58-5732-452b-ea44-f6dd6238bfca",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "t = PyTokenizer(5000)\n",
    "p = PreProcessor('', 'clean.py', t)\n",
    "x, y = p.get_training_data(50)\n",
    "\n",
    "m = Model(t.real_vocab_len, 50)\n",
    "m.train(x, y, 20, 64)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 6.9211 - accuracy: 0.1034\n",
      "Epoch 00001: loss improved from inf to 6.92105, saving model to best_model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 577s 2s/step - loss: 6.9211 - accuracy: 0.1034\n",
      "Epoch 2/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 6.1911 - accuracy: 0.1098\n",
      "Epoch 00002: loss improved from 6.92105 to 6.19113, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 588s 2s/step - loss: 6.1911 - accuracy: 0.1098\n",
      "Epoch 3/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 5.6462 - accuracy: 0.1098\n",
      "Epoch 00003: loss improved from 6.19113 to 5.64619, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 629s 2s/step - loss: 5.6462 - accuracy: 0.1098\n",
      "Epoch 4/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 5.2348 - accuracy: 0.1098\n",
      "Epoch 00004: loss improved from 5.64619 to 5.23476, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 588s 2s/step - loss: 5.2348 - accuracy: 0.1098\n",
      "Epoch 5/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.9472 - accuracy: 0.1098\n",
      "Epoch 00005: loss improved from 5.23476 to 4.94722, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 568s 2s/step - loss: 4.9472 - accuracy: 0.1098\n",
      "Epoch 6/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.7675 - accuracy: 0.1098\n",
      "Epoch 00006: loss improved from 4.94722 to 4.76749, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 569s 2s/step - loss: 4.7675 - accuracy: 0.1098\n",
      "Epoch 7/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.6683 - accuracy: 0.1098\n",
      "Epoch 00007: loss improved from 4.76749 to 4.66832, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 559s 2s/step - loss: 4.6683 - accuracy: 0.1098\n",
      "Epoch 8/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.6180 - accuracy: 0.1098\n",
      "Epoch 00008: loss improved from 4.66832 to 4.61804, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 563s 2s/step - loss: 4.6180 - accuracy: 0.1098\n",
      "Epoch 9/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.5961 - accuracy: 0.1098\n",
      "Epoch 00009: loss improved from 4.61804 to 4.59609, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 561s 2s/step - loss: 4.5961 - accuracy: 0.1098\n",
      "Epoch 10/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.5883 - accuracy: 0.1098\n",
      "Epoch 00010: loss improved from 4.59609 to 4.58826, saving model to best_model\n",
      "INFO:tensorflow:Assets written to: best_model/assets\n",
      "350/350 [==============================] - 568s 2s/step - loss: 4.5883 - accuracy: 0.1098\n",
      "Epoch 11/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.5883 - accuracy: 0.1098\n",
      "Epoch 00011: loss did not improve from 4.58826\n",
      "350/350 [==============================] - 550s 2s/step - loss: 4.5883 - accuracy: 0.1098\n",
      "Epoch 12/20\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.5925 - accuracy: 0.1098\n",
      "Epoch 00012: loss did not improve from 4.58826\n",
      "350/350 [==============================] - 551s 2s/step - loss: 4.5925 - accuracy: 0.1098\n",
      "Epoch 13/20\n",
      " 70/350 [=====>........................] - ETA: 7:09 - loss: 4.6394 - accuracy: 0.1076"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-0ea9abcb8830>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mm\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mModel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreal_vocab_len\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m50\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m20\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-7-542b4a05aaf6>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self, x, y, epochs, mini_batch_size)\u001B[0m\n\u001B[1;32m     36\u001B[0m                                      period=1)\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmini_batch_size\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcallbacks\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mgenerate_script\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mseed\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtemp\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36m_method_wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     64\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_method_wrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_in_multi_worker_mode\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mmethod\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0;31m# Running inside `run_distribute_coordinator` already.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m    846\u001B[0m                 batch_size=batch_size):\n\u001B[1;32m    847\u001B[0m               \u001B[0mcallbacks\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mon_train_batch_begin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 848\u001B[0;31m               \u001B[0mtmp_logs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtrain_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0miterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    849\u001B[0m               \u001B[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    850\u001B[0m               \u001B[0;31m# This blocks until the batch has finished executing.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    578\u001B[0m         \u001B[0mxla_context\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    579\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 580\u001B[0;31m       \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    581\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    582\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mtracing_count\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_tracing_count\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001B[0m in \u001B[0;36m_call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    609\u001B[0m       \u001B[0;31m# In this case we have created variables on the first call, so we run the\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    610\u001B[0m       \u001B[0;31m# defunned version which is guaranteed to never create variables.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 611\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateless_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=not-callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    612\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_stateful_fn\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    613\u001B[0m       \u001B[0;31m# Release the lock early so that multiple threads can perform the call\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   2418\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_lock\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2419\u001B[0m       \u001B[0mgraph_function\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_define_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2420\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mgraph_function\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_filtered_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# pylint: disable=protected-access\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2421\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2422\u001B[0m   \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_filtered_call\u001B[0;34m(self, args, kwargs)\u001B[0m\n\u001B[1;32m   1663\u001B[0m          if isinstance(t, (ops.Tensor,\n\u001B[1;32m   1664\u001B[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001B[0;32m-> 1665\u001B[0;31m         self.captured_inputs)\n\u001B[0m\u001B[1;32m   1666\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1667\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_flat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcaptured_inputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcancellation_manager\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36m_call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1744\u001B[0m       \u001B[0;31m# No tape is watching; skip to running the function.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1745\u001B[0m       return self._build_call_outputs(self._inference_function.call(\n\u001B[0;32m-> 1746\u001B[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001B[0m\u001B[1;32m   1747\u001B[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m         \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    596\u001B[0m               \u001B[0minputs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    597\u001B[0m               \u001B[0mattrs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mattrs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 598\u001B[0;31m               ctx=ctx)\n\u001B[0m\u001B[1;32m    599\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    600\u001B[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001B[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001B[0m in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     58\u001B[0m     \u001B[0mctx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mensure_initialized\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001B[0;32m---> 60\u001B[0;31m                                         inputs, attrs, num_outputs)\n\u001B[0m\u001B[1;32m     61\u001B[0m   \u001B[0;32mexcept\u001B[0m \u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     62\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nKa4jE-_a4pf",
    "colab_type": "code",
    "outputId": "ca582082-6d85-4b8c-ad67-5949b5cb9252",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    }
   },
   "source": [
    "!zip -r ./best_model.zip ./best_model/"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "updating: best_model/ (stored 0%)\n",
      "updating: best_model/variables/ (stored 0%)\n",
      "updating: best_model/variables/variables.index (deflated 68%)\n",
      "updating: best_model/variables/variables.data-00000-of-00001 (deflated 6%)\n",
      "updating: best_model/saved_model.pb (deflated 90%)\n",
      "updating: best_model/assets/ (stored 0%)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xjOVVozCcmn2",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "438905b6-3a4b-452a-9866-2acf0e04d7ff"
   },
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def generate_seed(tokenizer, sample_len):\n",
    "    \"\"\"\n",
    "    Get seed from cleaned file.\n",
    "    :param tokenizer: PyTokenizer, tokenizer to use.\n",
    "    :param sample_len: int, length of seed to generate\n",
    "    :return: np.array, model seed shape (1, sample_len)\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    with open('clean.py', 'r') as file:\n",
    "        while len(tokens) < sample_len:\n",
    "            tokens.extend(tokenizer.text_to_sequence(file.readline()))\n",
    "            print(len(tokens))\n",
    "\n",
    "    if len(tokens) != sample_len:\n",
    "        tokens = tokens[:sample_len]\n",
    "\n",
    "    return np.asarray(tokens, dtype=np.int).reshape((1, sample_len))\n",
    "\n",
    "m.model = load_model('best_model')\n",
    "print(m.sample_len)\n",
    "script = m.generate_script(generate_seed(t, m.sample_len), token=t.word_idx['EOF'])\n",
    "print(t.sequence_to_text(script))"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "50\n",
      "11\n",
      "19\n",
      "26\n",
      "38\n",
      "45\n",
      "54\n",
      "y loss = sample_weight_mode , add_metaclass = abc ( OOV _compile_weighted_metrics converted weighted_metrics , h5py ( . OOV _compile_weighted_metrics : if 3 , 'OOV : filter_sk_params ) Loss = fpath = reduction ' OOV _compile_weighted_metrics : kernel_size ) kwargs ( . OOV : = filter_sk_params ) \n",
      "'. ( OOV \n",
      " ' ', \n",
      " OOV ' OOV OOV ', , \n",
      " \n",
      " ) OOV , OOV \n",
      " OOV \n",
      " ( OOV OOV OOV : OOV : OOV \n",
      " OOV OOV ( ( OOV OOV ' \n",
      "'OOV . ' , ( OOV ) ( , OOV ( , in OOV '= , . \n",
      " ** ) OOV = ' ( : ( ( \n",
      ": OOV ) , 'OOV ) ( kwargs \n",
      " ' OOV \n",
      "OOV = OOV args OOV ( : OOV ( ( ( = ( ( OOV = OOV def OOV \n",
      ") OOV ) OOV OOV OOV OOV ', \n",
      " ( OOV OOV , self ( ' ( OOV , , , OOV OOV 'OOV \n",
      " OOV \n",
      " OOV ) OOV : ' OOV '' OOV \n",
      "        ( ( OOV ) OOV = None ) , , \n",
      "            ) OOV OOV ( , ) OOV [ = def = OOV , . ( = OOV OOV \n",
      "            ( OOV OOV \n",
      "            , \n",
      "            \n",
      "            ) ( ) = \n",
      "            \n",
      "            \n",
      "            return OOV OOV ( OOV OOV OOV ) ', . ] ' \n",
      "            ) '. = , OOV \n",
      " \n",
      " OOV OOV : \n",
      " \n",
      " \n",
      " ' \n",
      "                OOV \n",
      "                = OOV ( ) \n",
      "                    \n",
      "                    \n",
      "                    OOV ) 'return \n",
      " OOV . \n",
      " \n",
      " OOV ' ) ) OOV ) ) \n",
      "                    OOV ( OOV OOV '\n",
      " OOV OOV \n",
      " OOV , ) . ' OOV OOV [ . OOV OOV ', \n",
      " load_backend ' ( OOV OOV 'OOV = = OOV OOV ( = ) , \n",
      " ' '' OOV '\n",
      " OOV OOV ( OOV [ , \n",
      " , \n",
      " \n",
      " = ( , OOV , OOV ( : OOV , = == , OOV OOV OOV OOV OOV . \n",
      " , ( ' , ( \n",
      "                            'OOV , ) ) ' . , OOV OOV OOV \n",
      "                            \n",
      "                            OOV \n",
      "                            , = , \n",
      "                            ( ( '( OOV OOV OOV ) ' \n",
      "                            ( = ( OOV = , OOV OOV = ) OOV OOV OOV self , ', OOV ' OOV OOV ( , OOV : 'OOV , OOV OOV , def = OOV OOV ( ) = ' OOV return OOV OOV OOV OOV OOV \n",
      "                                : OOV ( = ) '\n",
      " , , ( OOV OOV ( . ' ( ( OOV ', OOV args \n",
      " OOV ' , \n",
      "                                    ) OOV OOV OOV '' \n",
      "                                    \n",
      "                                    OOV . OOV , ) '] \n",
      " ( \n",
      " \n",
      " , OOV = ( \n",
      " = ' OOV = = OOV OOV = OOV OOV OOV \n",
      "                                        \n",
      "                                        \n",
      "                                        ( OOV OOV \n",
      "                                        = \n",
      "                                        ( . OOV = ) , \n",
      "                                        \n",
      "                                        '. OOV ) OOV OOV . , OOV , \n",
      " ( ( ' ( ') . OOV ) OOV \n",
      " ( \n",
      " , \n",
      " ' \n",
      "                                            : , \n",
      "                                            OOV , OOV OOV , ') OOV OOV \n",
      " , OOV OOV ) ' np \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ONaKOmaTKY1B",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "outputId": "4f8f2afa-858d-4633-aad7-984156892d37"
   },
   "source": [
    "m.model = load_model('best_model')\n",
    "m.train(x, y, 20, 64)"
   ],
   "execution_count": 0,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/20\n",
      "219/350 [=================>............] - ETA: 3:20 - loss: 4.6172 - accuracy: 0.1092"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}